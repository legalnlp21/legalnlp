{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"extract_features_bert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM1LnmyeeklmC7jhRIdvuHQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"XAwdgk92weMT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628819981739,"user_tz":180,"elapsed":21953,"user":{"displayName":"Projeto NLP","photoUrl":"","userId":"01502388222153845214"}},"outputId":"d0e7e5b2-9288-4eb8-b606-ff01378c3704"},"source":["!pip install unidecode\n","!pip install transformers==4.2.2\n","!pip install pyreadr\n","!pip install unidecode\n","!pip install ftfy\n","!pip install git+https://github.com/legalnlp21/legalnlp"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n","\u001b[K     |████████████████████████████████| 241 kB 7.2 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.2.0\n","Collecting transformers==4.2.2\n","  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 8.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.41.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (4.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.2) (3.0.12)\n","Collecting tokenizers==0.9.4\n","  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 25.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.2) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.2) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.2) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.2) (1.15.0)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.2\n","Collecting pyreadr\n","  Downloading pyreadr-0.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (409 kB)\n","\u001b[K     |████████████████████████████████| 409 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>0.24.0 in /usr/local/lib/python3.7/dist-packages (from pyreadr) (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>0.24.0->pyreadr) (2.8.1)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>0.24.0->pyreadr) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>0.24.0->pyreadr) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>0.24.0->pyreadr) (1.15.0)\n","Installing collected packages: pyreadr\n","Successfully installed pyreadr-0.4.2\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Collecting ftfy\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=e3124daba954a1dadf7f171848aea2b459e736efe38ab1830a5a872828468f22\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","Successfully built ftfy\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.0.3\n","Collecting git+https://github.com/legalnlp21/legalnlp\n","  Cloning https://github.com/legalnlp21/legalnlp to /tmp/pip-req-build-4octg4hl\n","  Running command git clone -q https://github.com/legalnlp21/legalnlp /tmp/pip-req-build-4octg4hl\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from legalnlp==1.0.0) (6.0.3)\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->legalnlp==1.0.0) (0.2.5)\n","Building wheels for collected packages: legalnlp, wget\n","  Building wheel for legalnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for legalnlp: filename=legalnlp-1.0.0-py3-none-any.whl size=4242 sha256=b86bb7d3fe631def05100b026bc80d9acf483bbd11c9a4e5d2b90b06563ae93d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-6f7pktln/wheels/c9/d2/d7/488d600060896523461cecc4d9f49eec4efc02d494d7f04b62\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9673 sha256=cc01a2fe8be832bc95c81b24a0da8e2583249f87a42ea4587909ed4a032cade8\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built legalnlp wget\n","Installing collected packages: wget, legalnlp\n","Successfully installed legalnlp-1.0.0 wget-3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ujI89yVCwbVN","executionInfo":{"status":"ok","timestamp":1628820002997,"user_tz":180,"elapsed":4644,"user":{"displayName":"Projeto NLP","photoUrl":"","userId":"01502388222153845214"}}},"source":["from IPython.display import Image\n","from IPython.display import clear_output\n","import pickle\n","import random\n","import unidecode\n","import re\n","import ftfy\n","import os\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as tdata\n","import torch.optim as optim\n","import transformers\n","from transformers import AutoModel, AutoTokenizer, AutoConfig\n","from transformers import BertForPreTraining, BertModel, BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction, BertForQuestionAnswering\n","# manipulação numérica e de dataframes\n","import numpy as np\n","import pandas as pd\n","# gráficos e ajustes visuais\n","import matplotlib.pyplot as plt\n","import textwrap\n","from tqdm import tqdm"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulySGNf23lIy","executionInfo":{"status":"ok","timestamp":1628820017838,"user_tz":180,"elapsed":226,"user":{"displayName":"Projeto NLP","photoUrl":"","userId":"01502388222153845214"}}},"source":["# Import functions of legalnlp\n","from legalnlp.get_premodel import *\n","from legalnlp.clean_functions import *"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFyocCwQ43mI","executionInfo":{"status":"ok","timestamp":1628820137324,"user_tz":180,"elapsed":26395,"user":{"displayName":"Projeto NLP","photoUrl":"","userId":"01502388222153845214"}}},"source":["#Downloading BERTikal\n","get_premodel('bert')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NdMgDC3KvEeY","executionInfo":{"status":"ok","timestamp":1628820107976,"user_tz":180,"elapsed":372,"user":{"displayName":"Projeto NLP","photoUrl":"","userId":"01502388222153845214"}},"outputId":"66d1d37e-3e01-4015-89b2-7206b809d592"},"source":["%%time\n","data=pd.read_csv('https://raw.githubusercontent.com/legalnlp21/legalnlp/main/demo/data_base.csv')\n","data.drop(columns=['Unnamed: 0'],inplace=True)\n","data['text'] = data['text'].apply(lambda x:clean_bert(x))\n","model = '/content/BERTikal/BERTikal'\n","tokenizer= '/content/BERTikal/BERTikal/vocab.txt'\n","data"],"execution_count":9,"outputs":[{"output_type":"stream","text":["CPU times: user 395 ms, sys: 733 µs, total: 395 ms\n","Wall time: 455 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ni35GXIxu7kq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"92a9894c-3657-4611-88de-f383903d3681"},"source":["def extract_features_bert(path_model, path_tokenizer, data,gpu=True):\n","    if gpu:\n","        device = torch.device('cuda')\n","        os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","        bert_tokenizer = BertTokenizer.from_pretrained(path_tokenizer, do_lower_case=False)\n","        bert_model = BertModel.from_pretrained(path_model).to(device)\n","        bert = data.apply(lambda x: bert_tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True))\n","        data_text = list(data)\n","        encoded_inputs = bert_tokenizer(data_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n","        input_ids = encoded_inputs['input_ids'].to(device)\n","        clear_output()\n","        # Criando o nosso vetor de features\n","        features = []\n","    \n","        # Aplicando o modelo pré-treinado em cada frase e adicionando-o ao nosso vetor\n","    \n","        for i in tqdm(range(len(data)), mininterval=40, maxinterval=500):\n","    \n","            with torch.no_grad():\n","    \n","                last_hidden_states = bert_model(\n","                    input_ids[i:(i+1)])[1].cpu().numpy().reshape(-1).tolist()\n","            features.append(last_hidden_states)\n","    else:\n","        bert_tokenizer = BertTokenizer.from_pretrained(path_tokenizer, do_lower_case=False)\n","        bert_model = BertModel.from_pretrained(path_model)\n","        bert = data.apply(lambda x: bert_tokenizer.encode(\n","            x, add_special_tokens=True, max_length=512, truncation=True))\n","        encoded_inputs = bert_tokenizer(\n","            data_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n","        input_ids = encoded_inputs['input_ids']\n","        # Criando o nosso vetor de features\n","        features = []\n","    \n","        # Aplicando o modelo pré-treinado em cada frase e adicionando-o ao nosso vetor\n","    \n","        for i in tqdm(range(len(data))):\n","    \n","            with torch.no_grad():\n","    \n","                last_hidden_states = bert_model(\n","                    input_ids[i:(i+1)])[1].cpu().numpy().reshape(-1).tolist()\n","            features.append(last_hidden_states)\n","    features = np.array(features)\n","    features[:2]\n","    df_features = pd.DataFrame(features)\n","    return df_features\n","\n","\n","\n","    # os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","    #device = torch.device('cuda:0')\n","features= extract_features_bert(model,tokenizer,data['text'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 96%|█████████▌| 4884/5093 [03:20<00:08, 24.35it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"def2Xhv_ItWI"},"source":["features"],"execution_count":null,"outputs":[]}]}